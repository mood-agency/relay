//! CLI argument definitions and parsing

use clap::{Parser, Subcommand};
use std::path::PathBuf;

#[derive(Parser)]
#[command(author, version, about = "ðŸ¦€ Rohan - High-Performance OpenAPI Test Runner")]
pub struct Cli {
    #[command(subcommand)]
    pub command: Commands,
}

#[derive(Subcommand)]
pub enum Commands {
    /// Generate test plan (outputs JSON that can be fed to 'build')
    Plan(PlanArgs),
    /// Build k6 test scripts from a test plan JSON file
    Build(BuildArgs),
    /// Show instructions for running k6 tests
    Exec(ExecArgs),
    /// Validate an OpenAPI spec file
    Validate(ValidateArgs),
}

#[derive(clap::Args, Debug)]
pub struct PlanArgs {
    /// Path to the OpenAPI JSON specification file
    #[arg(required = true)]
    pub spec_path: PathBuf,

    /// Output file for the test plan JSON (default: test-plan.json)
    #[arg(long, short = 'o', default_value = "test-plan.json")]
    pub output: PathBuf,

    /// LLM API Base URL for custom endpoints (e.g., https://api.together.xyz/v1)
    /// If not set, genai auto-detects from model name and uses provider env vars
    #[arg(long, env = "ROHAN_API_BASE")]
    pub api_base: Option<String>,

    /// LLM Model Name (e.g., llama3-70b-8192, gpt-4o, claude-3-5-sonnet)
    #[arg(long, env = "ROHAN_MODEL", default_value = "llama3-70b-8192")]
    pub model: String,

    /// Number of parallel LLM workers for test generation (0 = unlimited)
    #[arg(long, short = 'w', default_value_t = 5)]
    pub workers: usize,

    /// Maximum requests per minute to the LLM API (0 = unlimited)
    #[arg(long, env = "ROHAN_RPM", default_value_t = 0)]
    pub rpm: u32,

    /// Enable verbose logging
    #[arg(long, default_value_t = false)]
    pub verbose: bool,

    /// Custom prompt directory (overrides embedded prompts)
    #[arg(long, env = "ROHAN_PROMPT_DIR")]
    pub prompt_dir: Option<String>,
}

#[derive(clap::Args, Debug)]
pub struct BuildArgs {
    /// Path to the test plan JSON file (generated by 'plan' command)
    #[arg(required = true)]
    pub plan_path: PathBuf,

    /// Output directory for generated test scripts
    #[arg(long, short = 'o', default_value = "tests/")]
    pub output: PathBuf,

    /// LLM API Base URL for custom endpoints (e.g., https://api.together.xyz/v1)
    /// If not set, genai auto-detects from model name and uses provider env vars
    #[arg(long, env = "ROHAN_API_BASE")]
    pub api_base: Option<String>,

    /// LLM Model Name (e.g., llama3-70b-8192, gpt-4o, claude-3-5-sonnet)
    #[arg(long, env = "ROHAN_MODEL", default_value = "llama3-70b-8192")]
    pub model: String,

    /// Number of parallel LLM workers for script generation (0 = unlimited)
    #[arg(long, short = 'w', default_value_t = 5)]
    pub workers: usize,

    /// Maximum requests per minute to the LLM API (0 = unlimited)
    #[arg(long, env = "ROHAN_RPM", default_value_t = 0)]
    pub rpm: u32,

    /// Enable verbose logging
    #[arg(long, default_value_t = false)]
    pub verbose: bool,

    /// Custom prompt directory (overrides embedded prompts)
    #[arg(long, env = "ROHAN_PROMPT_DIR")]
    pub prompt_dir: Option<String>,
}

#[derive(clap::Args, Debug)]
pub struct ExecArgs {
    /// Directory containing k6 test scripts
    #[arg(required = true)]
    pub tests_dir: PathBuf,

    /// Target Base URL for the API (used in example commands)
    #[arg(long, default_value = "http://localhost:8080")]
    pub target: Option<String>,
}

#[derive(clap::Args, Debug)]
pub struct ValidateArgs {
    /// Path to the OpenAPI JSON specification file
    #[arg(required = true)]
    pub spec_path: PathBuf,
}

