//! LLM-based test plan and script generation

use anyhow::{Context, Result};
use genai::adapter::AdapterKind;
use genai::chat::{ChatMessage, ChatRequest};
use genai::resolver::{AuthData, Endpoint, ServiceTargetResolver};
use genai::{Client, ClientBuilder, ModelIden, ServiceTarget};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;

// Embed default prompts at compile time
const DEFAULT_PLANNER_SYSTEM: &str = include_str!("../prompts/planner_system.md");
const DEFAULT_PLANNER_USER: &str = include_str!("../prompts/planner_user.md");
const DEFAULT_BUILDER_SYSTEM: &str = include_str!("../prompts/builder_system.md");
const DEFAULT_BUILDER_USER: &str = include_str!("../prompts/builder_user.md");

// Batch prompts for processing multiple items per request
const DEFAULT_PLANNER_BATCH_SYSTEM: &str = include_str!("../prompts/planner_batch_system.md");
const DEFAULT_PLANNER_BATCH_USER: &str = include_str!("../prompts/planner_batch_user.md");
const DEFAULT_BUILDER_BATCH_SYSTEM: &str = include_str!("../prompts/builder_batch_system.md");
const DEFAULT_BUILDER_BATCH_USER: &str = include_str!("../prompts/builder_batch_user.md");

// E2E prompts for workflow testing
const DEFAULT_E2E_PLANNER_SYSTEM: &str = include_str!("../prompts/e2e_planner_system.md");
const DEFAULT_E2E_PLANNER_USER: &str = include_str!("../prompts/e2e_planner_user.md");
const DEFAULT_E2E_BUILDER_SYSTEM: &str = include_str!("../prompts/e2e_builder_system.md");
const DEFAULT_E2E_BUILDER_USER: &str = include_str!("../prompts/e2e_builder_user.md");


/// A structured test scenario generated by the LLM
#[allow(dead_code)]
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestScenario {
    pub name: String,
    pub steps: Vec<String>,
    #[serde(default = "default_priority")]
    pub priority: String,
    #[serde(default)]
    pub tags: Vec<String>,
}

#[allow(dead_code)]
fn default_priority() -> String {
    "medium".to_string()
}

/// A single test entry in the test plan
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestEntry {
    pub name: String,
    pub method: String,
    pub path: String,
    pub endpoint_spec: serde_json::Value,
}

/// An E2E test scenario with multiple steps
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct E2EScenario {
    pub name: String,
    pub steps: Vec<String>,
    pub description: String,
    /// Combined endpoint specs relevant to this scenario
    #[serde(default)]
    pub endpoint_spec: serde_json::Value,
}

/// The complete test plan that can be saved to JSON and loaded by the build phase
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestPlan {
    /// Version of the test plan format
    pub version: String,
    /// Original OpenAPI spec path
    pub spec_path: String,
    /// API title from the spec
    pub api_title: String,
    /// API version from the spec
    pub api_version: String,
    /// Whether this is an E2E test plan
    #[serde(default)]
    pub e2e: bool,
    /// List of test entries (for unit tests)
    #[serde(default)]
    pub tests: Vec<TestEntry>,
    /// List of E2E scenarios (for E2E tests)
    #[serde(default)]
    pub scenarios: Vec<E2EScenario>,
}

impl TestPlan {
    pub fn new(spec_path: &str, api_title: &str, api_version: &str, e2e: bool) -> Self {
        Self {
            version: "1.0".to_string(),
            spec_path: spec_path.to_string(),
            api_title: api_title.to_string(),
            api_version: api_version.to_string(),
            e2e,
            tests: Vec::new(),
            scenarios: Vec::new(),
        }
    }

    /// Save the test plan to a JSON file
    pub fn save(&self, path: &Path) -> Result<()> {
        let json = serde_json::to_string_pretty(self)?;
        fs::write(path, json)?;
        Ok(())
    }

    /// Load a test plan from a JSON file
    pub fn load(path: &Path) -> Result<Self> {
        let content = fs::read_to_string(path)
            .with_context(|| format!("Failed to read test plan from {:?}", path))?;
        let plan: TestPlan = serde_json::from_str(&content)
            .with_context(|| "Failed to parse test plan JSON")?;
        Ok(plan)
    }
}

/// Prompt loader that supports custom prompt directories
pub struct PromptLoader {
    prompt_dir: Option<String>,
}

impl PromptLoader {
    pub fn new(prompt_dir: Option<String>) -> Self {
        Self { prompt_dir }
    }

    /// Load a prompt, preferring custom file over embedded default
    fn load_prompt(&self, filename: &str, default: &str) -> String {
        if let Some(ref dir) = self.prompt_dir {
            let path = Path::new(dir).join(filename);
            if path.exists() {
                if let Ok(content) = fs::read_to_string(&path) {
                    return content;
                }
            }
        }
        default.to_string()
    }

    pub fn planner_system(&self) -> String {
        self.load_prompt("planner_system.md", DEFAULT_PLANNER_SYSTEM)
    }

    pub fn planner_user(&self) -> String {
        self.load_prompt("planner_user.md", DEFAULT_PLANNER_USER)
    }

    pub fn builder_system(&self) -> String {
        self.load_prompt("builder_system.md", DEFAULT_BUILDER_SYSTEM)
    }

    pub fn builder_user(&self) -> String {
        self.load_prompt("builder_user.md", DEFAULT_BUILDER_USER)
    }

    // Batch prompts
    pub fn planner_batch_system(&self) -> String {
        self.load_prompt("planner_batch_system.md", DEFAULT_PLANNER_BATCH_SYSTEM)
    }

    pub fn planner_batch_user(&self) -> String {
        self.load_prompt("planner_batch_user.md", DEFAULT_PLANNER_BATCH_USER)
    }

    pub fn builder_batch_system(&self) -> String {
        self.load_prompt("builder_batch_system.md", DEFAULT_BUILDER_BATCH_SYSTEM)
    }

    pub fn builder_batch_user(&self) -> String {
        self.load_prompt("builder_batch_user.md", DEFAULT_BUILDER_BATCH_USER)
    }

    // E2E prompts
    pub fn e2e_planner_system(&self) -> String {
        self.load_prompt("e2e_planner_system.md", DEFAULT_E2E_PLANNER_SYSTEM)
    }

    pub fn e2e_planner_user(&self) -> String {
        self.load_prompt("e2e_planner_user.md", DEFAULT_E2E_PLANNER_USER)
    }

    pub fn e2e_builder_system(&self) -> String {
        self.load_prompt("e2e_builder_system.md", DEFAULT_E2E_BUILDER_SYSTEM)
    }

    pub fn e2e_builder_user(&self) -> String {
        self.load_prompt("e2e_builder_user.md", DEFAULT_E2E_BUILDER_USER)
    }

}

/// Simple template replacement
fn render_template(template: &str, vars: &[(&str, &str)]) -> String {
    let mut result = template.to_string();
    for (key, value) in vars {
        result = result.replace(&format!("{{{{{}}}}}", key), value);
    }
    result
}

/// Detect the env var name from an API base URL
fn env_var_for_api_base(api_base: &str) -> &'static str {
    let base_lower = api_base.to_lowercase();
    if base_lower.contains("groq.com") {
        "GROQ_API_KEY"
    } else if base_lower.contains("together.xyz") || base_lower.contains("together.ai") {
        "TOGETHER_API_KEY"
    } else if base_lower.contains("fireworks.ai") {
        "FIREWORKS_API_KEY"
    } else if base_lower.contains("anthropic.com") {
        "ANTHROPIC_API_KEY"
    } else if base_lower.contains("x.ai") || base_lower.contains("xai.com") {
        "XAI_API_KEY"
    } else if base_lower.contains("deepseek.com") {
        "DEEPSEEK_API_KEY"
    } else if base_lower.contains("mistral.ai") {
        "MISTRAL_API_KEY"
    } else {
        // Default to OPENAI_API_KEY for OpenAI and unknown providers
        "OPENAI_API_KEY"
    }
}

/// Build a genai client.
/// - If api_base is Some, creates a custom resolver for that endpoint (OpenAI-compatible).
/// - If api_base is None, uses genai's default auto-detection from model name.
fn build_client(api_base: Option<&str>) -> Client {
    match api_base {
        Some(base) => {
            // Custom endpoint - detect env var from URL
            let env_var = env_var_for_api_base(base);
            let api_base = base.to_string();

            let target_resolver = ServiceTargetResolver::from_resolver_fn(
                move |service_target: ServiceTarget| -> Result<ServiceTarget, genai::resolver::Error> {
                    let endpoint = Endpoint::from_owned(api_base.clone());
                    let auth = AuthData::from_env(env_var);
                    // Force OpenAI adapter for custom endpoints (OpenAI-compatible API)
                    let model = ModelIden::new(AdapterKind::OpenAI, service_target.model.model_name);
                    Ok(ServiceTarget {
                        endpoint,
                        auth,
                        model,
                    })
                },
            );

            ClientBuilder::default()
                .with_service_target_resolver(target_resolver)
                .build()
        }
        None => {
            // Let genai auto-detect provider from model name
            // Auth is automatically loaded from provider-specific env vars:
            // GROQ_API_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY, etc.
            Client::default()
        }
    }
}

/// Represents a single API endpoint extracted from OpenAPI spec
#[derive(Debug, Clone)]
struct EndpointInfo {
    path: String,
    method: String,
    spec: serde_json::Value,
}

/// Extract individual endpoints from an OpenAPI spec
fn extract_endpoints(spec: &str) -> Result<Vec<EndpointInfo>> {
    let spec_json: serde_json::Value = serde_json::from_str(spec)
        .with_context(|| "Failed to parse OpenAPI spec as JSON")?;

    let paths = spec_json
        .get("paths")
        .and_then(|p| p.as_object())
        .with_context(|| "OpenAPI spec missing 'paths' object")?;

    let mut endpoints = Vec::new();

    for (path, path_item) in paths {
        if let Some(path_obj) = path_item.as_object() {
            for (method, operation) in path_obj {
                // Skip non-HTTP method keys like "parameters", "summary", etc.
                let http_methods = ["get", "post", "put", "patch", "delete", "head", "options"];
                if !http_methods.contains(&method.to_lowercase().as_str()) {
                    continue;
                }

                // Build a minimal spec for this endpoint
                let endpoint_spec = serde_json::json!({
                    "path": path,
                    "method": method.to_uppercase(),
                    "operation": operation
                });

                endpoints.push(EndpointInfo {
                    path: path.clone(),
                    method: method.to_uppercase(),
                    spec: endpoint_spec,
                });
            }
        }
    }

    Ok(endpoints)
}

/// Parse LLM response to extract test names
fn parse_test_names(content: &str) -> Result<Vec<String>> {
    // Clean up any markdown fences that might be present
    let clean_json = content
        .trim()
        .trim_start_matches("```json")
        .trim_start_matches("```")
        .trim_end_matches("```")
        .trim();

    // Parse as array of strings
    let names: Vec<String> = serde_json::from_str(clean_json)
        .with_context(|| format!("Failed to parse LLM response as JSON array: {}", content))?;

    Ok(names)
}

/// Parse batched LLM response for planning (endpoint_id -> test names)
fn parse_batched_test_names(content: &str) -> Result<std::collections::HashMap<String, Vec<String>>> {
    let clean_json = content
        .trim()
        .trim_start_matches("```json")
        .trim_start_matches("```")
        .trim_end_matches("```")
        .trim();

    let result: std::collections::HashMap<String, Vec<String>> = serde_json::from_str(clean_json)
        .with_context(|| format!("Failed to parse batched LLM response as JSON object: {}", content))?;

    Ok(result)
}

/// Parse batched LLM response for building (test_name -> code)
fn parse_batched_scripts(content: &str) -> Result<std::collections::HashMap<String, String>> {
    let clean_json = content
        .trim()
        .trim_start_matches("```json")
        .trim_start_matches("```")
        .trim_end_matches("```")
        .trim();

    let result: std::collections::HashMap<String, String> = serde_json::from_str(clean_json)
        .with_context(|| format!("Failed to parse batched scripts response as JSON object: {}", content))?;

    Ok(result)
}

/// Result of planning for a single endpoint (for TestPlan generation)
struct PlanResult {
    method: String,
    path: String,
    endpoint_spec: serde_json::Value,
    test_names: Result<Vec<String>>,
}

/// Create a TestPlan from an OpenAPI spec using an LLM
/// This generates a JSON-serializable plan that can be saved and used by the build phase
///
/// # Arguments
/// * `spec` - The OpenAPI specification as a JSON string
/// * `spec_path` - Path to the original spec file (for reference)
/// * `api_base` - Optional custom API base URL
/// * `model_name` - The LLM model to use
/// * `prompt_dir` - Optional custom prompt directory
/// * `workers` - Number of parallel workers (0 = unlimited)
/// * `rpm` - Maximum requests per minute (0 = unlimited)
/// * `batch_size` - Number of endpoints to batch per request (1 = no batching)
/// * `e2e` - Whether to generate E2E workflow tests instead of unit tests
pub fn create_test_plan(
    spec: &str,
    spec_path: &str,
    api_base: Option<&str>,
    model_name: &str,
    prompt_dir: Option<String>,
    workers: usize,
    rpm: u32,
    batch_size: usize,
    e2e: bool,
) -> Result<TestPlan> {
    let client = build_client(api_base);
    let loader = PromptLoader::new(prompt_dir.clone());

    // Parse spec to get API info
    let spec_json: serde_json::Value = serde_json::from_str(spec)
        .with_context(|| "Failed to parse OpenAPI spec as JSON")?;
    
    let api_title = spec_json
        .get("info")
        .and_then(|i| i.get("title"))
        .and_then(|t| t.as_str())
        .unwrap_or("Unknown API")
        .to_string();
    
    let api_version = spec_json
        .get("info")
        .and_then(|i| i.get("version"))
        .and_then(|v| v.as_str())
        .unwrap_or("0.0.0")
        .to_string();

    // E2E mode processes the entire spec at once to identify cross-endpoint workflows
    if e2e {
        return create_e2e_test_plan(
            spec,
            spec_path,
            &api_title,
            &api_version,
            &client,
            &loader,
            model_name,
        );
    }

    // Extract all endpoints from the spec
    let endpoints = extract_endpoints(spec)?;
    let endpoint_count = endpoints.len();

    // Determine effective batch size (1 = no batching)
    let effective_batch_size = if batch_size == 0 { 1 } else { batch_size };
    let use_batching = effective_batch_size > 1;

    // Calculate number of batches
    let num_batches = (endpoint_count + effective_batch_size - 1) / effective_batch_size;

    // Determine effective worker count (based on batches, not endpoints)
    let effective_workers = if workers == 0 { num_batches } else { workers.min(num_batches) };

    println!("üìã Found {} endpoints to process", endpoint_count);
    if use_batching {
        println!("üì¶ Batching: {} endpoints per request ({} batches)", effective_batch_size, num_batches);
    }
    println!("üë∑ Using {} parallel workers", effective_workers);
    if rpm > 0 {
        println!("‚è±Ô∏è  Rate limit: {} requests/minute", rpm);
    }
    println!();

    // Print what we're about to process
    for endpoint in &endpoints {
        println!("   ‚Ä¢ {} {}", endpoint.method, endpoint.path);
    }
    println!();

    let rt = tokio::runtime::Runtime::new()?;

    let (test_entries, success_count, fail_count) = if use_batching {
        // Batched processing
        rt.block_on(process_endpoints_batched(
            &client,
            &loader,
            model_name,
            endpoints,
            effective_batch_size,
            effective_workers,
            rpm,
        ))
    } else {
        // Individual processing (original behavior)
        rt.block_on(process_endpoints_individual(
            &client,
            &loader,
            model_name,
            endpoints,
            effective_workers,
            rpm,
        ))
    };

    // Build the test plan
    let mut plan = TestPlan::new(spec_path, &api_title, &api_version, false);
    plan.tests = test_entries;

    println!(
        "\nüìä Summary: {} endpoints succeeded, {} failed",
        success_count, fail_count
    );
    println!("‚úÖ Total: {} test entries in plan", plan.tests.len());

    Ok(plan)
}

/// Create an E2E test plan by analyzing the entire API spec for workflow patterns
fn create_e2e_test_plan(
    spec: &str,
    spec_path: &str,
    api_title: &str,
    api_version: &str,
    client: &Client,
    loader: &PromptLoader,
    model_name: &str,
) -> Result<TestPlan> {
    println!("üîó E2E Mode: Analyzing full API spec for workflow patterns...");

    let system_prompt = loader.e2e_planner_system();
    let user_template = loader.e2e_planner_user();
    let user_prompt = render_template(&user_template, &[("spec", spec)]);

    let rt = tokio::runtime::Runtime::new()?;
    
    let result = rt.block_on(async {
        let chat_req = ChatRequest::new(vec![
            ChatMessage::system(&system_prompt),
            ChatMessage::user(user_prompt),
        ]);

        client.exec_chat(model_name, chat_req, None).await
    });

    let scenarios = match result {
        Ok(response) => {
            let content = response.first_text().unwrap_or_default();
            parse_e2e_scenarios(&content, spec)?
        }
        Err(e) => {
            return Err(anyhow::anyhow!("E2E planning failed: {}", e));
        }
    };

    let mut plan = TestPlan::new(spec_path, api_title, api_version, true);
    plan.scenarios = scenarios;

    println!("\n‚úÖ Generated {} E2E workflow scenarios", plan.scenarios.len());
    for scenario in &plan.scenarios {
        println!("   ‚Ä¢ {} ({} steps)", scenario.name, scenario.steps.len());
    }

    Ok(plan)
}

/// Parse E2E scenarios from LLM response
fn parse_e2e_scenarios(content: &str, spec: &str) -> Result<Vec<E2EScenario>> {
    let clean_json = content
        .trim()
        .trim_start_matches("```json")
        .trim_start_matches("```")
        .trim_end_matches("```")
        .trim();

    let mut scenarios: Vec<E2EScenario> = serde_json::from_str(clean_json)
        .with_context(|| format!("Failed to parse E2E scenarios as JSON: {}", content))?;

    // Attach the full spec to each scenario for use during build phase
    let spec_json: serde_json::Value = serde_json::from_str(spec).unwrap_or_default();
    for scenario in &mut scenarios {
        scenario.endpoint_spec = spec_json.clone();
    }

    Ok(scenarios)
}

/// Process endpoints individually (one LLM call per endpoint)
async fn process_endpoints_individual(
    client: &Client,
    loader: &PromptLoader,
    model_name: &str,
    endpoints: Vec<EndpointInfo>,
    effective_workers: usize,
    rpm: u32,
) -> (Vec<TestEntry>, usize, usize) {
    use std::sync::Arc;
    use tokio::sync::{mpsc, Mutex, Semaphore};
    use tokio::time::{Duration, Instant};

    let system_prompt = loader.planner_system();
    let user_template = loader.planner_user();

    let semaphore = Arc::new(Semaphore::new(effective_workers));
    
    let rate_limiter = if rpm > 0 {
        let interval_ms = 60_000 / rpm as u64;
        Some(Arc::new(Mutex::new((Instant::now(), interval_ms))))
    } else {
        None
    };

    let (tx, mut rx) = mpsc::unbounded_channel::<PlanResult>();
    let mut handles = Vec::new();

    for endpoint in endpoints {
        let client = client.clone();
        let model = model_name.to_string();
        let system = system_prompt.clone();
        let template = user_template.clone();
        let ep_method = endpoint.method.clone();
        let ep_path = endpoint.path.clone();
        let ep_spec = endpoint.spec.clone();
        let sem = Arc::clone(&semaphore);
        let rl = rate_limiter.clone();
        let tx = tx.clone();

        let handle = tokio::spawn(async move {
            let _permit = sem.acquire().await.unwrap();

            if let Some(ref limiter) = rl {
                let mut state = limiter.lock().await;
                let (last_time, interval_ms) = *state;
                let elapsed = last_time.elapsed().as_millis() as u64;
                if elapsed < interval_ms {
                    tokio::time::sleep(Duration::from_millis(interval_ms - elapsed)).await;
                }
                *state = (Instant::now(), interval_ms);
            }

            let endpoint_spec_str = serde_json::to_string_pretty(&ep_spec)
                .unwrap_or_else(|_| "{}".to_string());
            let user_prompt = render_template(&template, &[("spec", &endpoint_spec_str)]);

            let chat_req = ChatRequest::new(vec![
                ChatMessage::system(&system),
                ChatMessage::user(user_prompt),
            ]);

            let result = client.exec_chat(&model, chat_req, None).await;

            let test_names = match result {
                Ok(response) => {
                    let content = response.first_text().unwrap_or_default();
                    parse_test_names(&content)
                }
                Err(e) => Err(anyhow::anyhow!("LLM call failed: {}", e)),
            };

            let _ = tx.send(PlanResult {
                method: ep_method,
                path: ep_path,
                endpoint_spec: ep_spec,
                test_names,
            });
        });

        handles.push(handle);
    }

    drop(tx);

    let mut test_entries = Vec::new();
    let mut success_count = 0;
    let mut fail_count = 0;

    while let Some(result) = rx.recv().await {
        match result.test_names {
            Ok(names) => {
                println!(
                    "‚úÖ {} {} - {} tests",
                    result.method,
                    result.path,
                    names.len()
                );
                success_count += 1;
                for name in names {
                    test_entries.push(TestEntry {
                        name,
                        method: result.method.clone(),
                        path: result.path.clone(),
                        endpoint_spec: result.endpoint_spec.clone(),
                    });
                }
            }
            Err(e) => {
                println!("‚ùå {} {} - Error: {}", result.method, result.path, e);
                fail_count += 1;
            }
        }
    }

    for handle in handles {
        let _ = handle.await;
    }

    (test_entries, success_count, fail_count)
}

/// A batch of endpoints to process together
#[derive(Clone)]
struct EndpointBatch {
    /// Unique ID for each endpoint in the batch (for mapping results)
    endpoints: Vec<(String, EndpointInfo)>,
}

/// Result of processing a batch of endpoints
struct BatchPlanResult {
    /// The batch that was processed
    batch: EndpointBatch,
    /// Results mapped by endpoint_id
    results: Result<std::collections::HashMap<String, Vec<String>>>,
}

/// Process endpoints in batches (multiple endpoints per LLM call)
async fn process_endpoints_batched(
    client: &Client,
    loader: &PromptLoader,
    model_name: &str,
    endpoints: Vec<EndpointInfo>,
    batch_size: usize,
    effective_workers: usize,
    rpm: u32,
) -> (Vec<TestEntry>, usize, usize) {
    use std::sync::Arc;
    use tokio::sync::{mpsc, Mutex, Semaphore};
    use tokio::time::{Duration, Instant};

    let system_prompt = loader.planner_batch_system();
    let user_template = loader.planner_batch_user();

    // Create batches with unique IDs for each endpoint
    let mut batches: Vec<EndpointBatch> = Vec::new();
    let mut current_batch: Vec<(String, EndpointInfo)> = Vec::new();
    
    for (idx, endpoint) in endpoints.into_iter().enumerate() {
        let endpoint_id = format!("endpoint_{}", idx);
        current_batch.push((endpoint_id, endpoint));
        
        if current_batch.len() >= batch_size {
            batches.push(EndpointBatch { endpoints: current_batch });
            current_batch = Vec::new();
        }
    }
    
    // Don't forget the last partial batch
    if !current_batch.is_empty() {
        batches.push(EndpointBatch { endpoints: current_batch });
    }

    let semaphore = Arc::new(Semaphore::new(effective_workers));
    
    let rate_limiter = if rpm > 0 {
        let interval_ms = 60_000 / rpm as u64;
        Some(Arc::new(Mutex::new((Instant::now(), interval_ms))))
    } else {
        None
    };

    let (tx, mut rx) = mpsc::unbounded_channel::<BatchPlanResult>();
    let mut handles = Vec::new();

    for batch in batches {
        let client = client.clone();
        let model = model_name.to_string();
        let system = system_prompt.clone();
        let template = user_template.clone();
        let sem = Arc::clone(&semaphore);
        let rl = rate_limiter.clone();
        let tx = tx.clone();
        let batch_clone = batch.clone();

        let handle = tokio::spawn(async move {
            let _permit = sem.acquire().await.unwrap();

            if let Some(ref limiter) = rl {
                let mut state = limiter.lock().await;
                let (last_time, interval_ms) = *state;
                let elapsed = last_time.elapsed().as_millis() as u64;
                if elapsed < interval_ms {
                    tokio::time::sleep(Duration::from_millis(interval_ms - elapsed)).await;
                }
                *state = (Instant::now(), interval_ms);
            }

            // Build the endpoints JSON for the batch
            let endpoints_json: Vec<serde_json::Value> = batch.endpoints.iter()
                .map(|(id, ep)| {
                    serde_json::json!({
                        "endpoint_id": id,
                        "method": ep.method,
                        "path": ep.path,
                        "spec": ep.spec
                    })
                })
                .collect();
            
            let endpoints_str = serde_json::to_string_pretty(&endpoints_json)
                .unwrap_or_else(|_| "[]".to_string());
            
            let user_prompt = render_template(&template, &[("endpoints", &endpoints_str)]);

            let chat_req = ChatRequest::new(vec![
                ChatMessage::system(&system),
                ChatMessage::user(user_prompt),
            ]);

            let result = client.exec_chat(&model, chat_req, None).await;

            let results = match result {
                Ok(response) => {
                    let content = response.first_text().unwrap_or_default();
                    parse_batched_test_names(&content)
                }
                Err(e) => Err(anyhow::anyhow!("LLM call failed: {}", e)),
            };

            let _ = tx.send(BatchPlanResult {
                batch: batch_clone,
                results,
            });
        });

        handles.push(handle);
    }

    drop(tx);

    let mut test_entries = Vec::new();
    let mut success_count = 0;
    let mut fail_count = 0;

    while let Some(batch_result) = rx.recv().await {
        match batch_result.results {
            Ok(results_map) => {
                // Process each endpoint in the batch
                for (endpoint_id, endpoint_info) in &batch_result.batch.endpoints {
                    if let Some(test_names) = results_map.get(endpoint_id) {
                        println!(
                            "‚úÖ {} {} - {} tests",
                            endpoint_info.method,
                            endpoint_info.path,
                            test_names.len()
                        );
                        success_count += 1;
                        for name in test_names {
                            test_entries.push(TestEntry {
                                name: name.clone(),
                                method: endpoint_info.method.clone(),
                                path: endpoint_info.path.clone(),
                                endpoint_spec: endpoint_info.spec.clone(),
                            });
                        }
                    } else {
                        // Endpoint was missing from response
                        println!(
                            "‚ö†Ô∏è  {} {} - Missing from batch response",
                            endpoint_info.method,
                            endpoint_info.path
                        );
                        fail_count += 1;
                    }
                }
            }
            Err(e) => {
                // Entire batch failed
                println!("‚ùå Batch failed ({} endpoints): {}", batch_result.batch.endpoints.len(), e);
                for (_, endpoint_info) in &batch_result.batch.endpoints {
                    println!("   ‚Ä¢ {} {}", endpoint_info.method, endpoint_info.path);
                }
                fail_count += batch_result.batch.endpoints.len();
            }
        }
    }

    for handle in handles {
        let _ = handle.await;
    }

    (test_entries, success_count, fail_count)
}

/// Result of generating a single test script
struct ScriptResult {
    test_name: String,
    code: Result<String>,
}

/// A batch of tests to generate scripts for
#[derive(Clone)]
struct TestBatch {
    /// Tests in this batch with their endpoint specs
    tests: Vec<(String, serde_json::Value)>,
}

/// Result of processing a batch of test scripts
struct BatchScriptResult {
    /// The batch that was processed
    batch: TestBatch,
    /// Results mapped by test_name
    results: Result<std::collections::HashMap<String, String>>,
}

/// Callback type for handling generated scripts
/// Parameters: (test_name, code)
/// Returns: Ok(true) if written, Ok(false) if skipped, Err on failure
pub type ScriptCallback = Box<dyn Fn(&str, &str) -> Result<bool> + Send + Sync>;

/// Build test scripts from a TestPlan
/// 
/// # Arguments
/// * `plan` - The test plan to generate scripts from
/// * `api_base` - Optional custom API base URL
/// * `model_name` - The LLM model to use
/// * `prompt_dir` - Optional custom prompt directory
/// * `workers` - Number of parallel workers (0 = unlimited)
/// * `rpm` - Maximum requests per minute (0 = unlimited)
/// * `batch_size` - Number of tests to batch per request (1 = no batching)
/// * `on_script` - Optional callback called for each generated script (writes files incrementally)
pub fn build_scripts_from_plan(
    plan: &TestPlan,
    api_base: Option<&str>,
    model_name: &str,
    prompt_dir: Option<String>,
    workers: usize,
    rpm: u32,
    batch_size: usize,
    on_script: Option<ScriptCallback>,
) -> Result<Vec<(String, String)>> {
    let client = build_client(api_base);
    let loader = PromptLoader::new(prompt_dir);

    // E2E mode uses scenarios instead of individual tests
    if plan.e2e {
        return build_e2e_scripts_from_plan(
            plan,
            &client,
            &loader,
            model_name,
            workers,
            rpm,
            batch_size,
            on_script,
        );
    }

    let test_count = plan.tests.len();
    
    // Determine effective batch size (1 = no batching)
    let effective_batch_size = if batch_size == 0 { 1 } else { batch_size };
    let use_batching = effective_batch_size > 1;

    // Calculate number of batches
    let num_batches = (test_count + effective_batch_size - 1) / effective_batch_size;

    let effective_workers = if workers == 0 {
        num_batches
    } else {
        workers.min(num_batches)
    };

    println!("\nüèóÔ∏è  Building {} test scripts", test_count);
    if use_batching {
        println!("üì¶ Batching: {} tests per request ({} batches)", effective_batch_size, num_batches);
    }
    println!("üë∑ Using {} parallel workers", effective_workers);
    if rpm > 0 {
        println!("‚è±Ô∏è  Rate limit: {} requests/minute", rpm);
    }
    println!();

    let rt = tokio::runtime::Runtime::new()?;

    let (scripts, success_count, fail_count, written_count, skipped_count) = if use_batching {
        rt.block_on(build_scripts_batched(
            &client,
            &loader,
            model_name,
            plan,
            effective_batch_size,
            effective_workers,
            rpm,
            &on_script,
        ))
    } else {
        rt.block_on(build_scripts_individual(
            &client,
            &loader,
            model_name,
            plan,
            effective_workers,
            rpm,
            &on_script,
        ))
    };

    println!(
        "\nüìä Scripts: {} generated, {} failed",
        success_count, fail_count
    );
    
    if on_script.is_some() {
        println!("üíæ Files: {} written, {} skipped", written_count, skipped_count);
    }

    Ok(scripts)
}

/// Build E2E test scripts from scenarios in the plan
fn build_e2e_scripts_from_plan(
    plan: &TestPlan,
    client: &Client,
    loader: &PromptLoader,
    model_name: &str,
    workers: usize,
    rpm: u32,
    _batch_size: usize,
    on_script: Option<ScriptCallback>,
) -> Result<Vec<(String, String)>> {
    use std::sync::Arc;
    use tokio::sync::{mpsc, Mutex, Semaphore};
    use tokio::time::{Duration, Instant};

    let scenario_count = plan.scenarios.len();
    
    let effective_workers = if workers == 0 {
        scenario_count.max(1)
    } else {
        workers.min(scenario_count.max(1))
    };

    println!("\nüîó Building {} E2E test scripts", scenario_count);
    println!("üë∑ Using {} parallel workers", effective_workers);
    if rpm > 0 {
        println!("‚è±Ô∏è  Rate limit: {} requests/minute", rpm);
    }
    println!();

    let rt = tokio::runtime::Runtime::new()?;

    let system_prompt = loader.e2e_builder_system();
    let user_template = loader.e2e_builder_user();

    let semaphore = Arc::new(Semaphore::new(effective_workers));
    
    let rate_limiter = if rpm > 0 {
        let interval_ms = 60_000 / rpm as u64;
        Some(Arc::new(Mutex::new((Instant::now(), interval_ms))))
    } else {
        None
    };

    struct E2EScriptResult {
        name: String,
        code: Result<String>,
    }

    let (scripts, success_count, fail_count, written_count, skipped_count) = rt.block_on(async {
        let (tx, mut rx) = mpsc::unbounded_channel::<E2EScriptResult>();
        let mut handles = Vec::new();

        for scenario in &plan.scenarios {
            let client = client.clone();
            let model = model_name.to_string();
            let system = system_prompt.clone();
            let template = user_template.clone();
            let scenario_name = scenario.name.clone();
            let steps = scenario.steps.clone();
            let description = scenario.description.clone();
            let endpoint_spec = scenario.endpoint_spec.clone();
            let sem = Arc::clone(&semaphore);
            let rl = rate_limiter.clone();
            let tx = tx.clone();

            let handle = tokio::spawn(async move {
                let _permit = sem.acquire().await.unwrap();

                if let Some(ref limiter) = rl {
                    let mut state = limiter.lock().await;
                    let (last_time, interval_ms) = *state;
                    let elapsed = last_time.elapsed().as_millis() as u64;
                    if elapsed < interval_ms {
                        tokio::time::sleep(Duration::from_millis(interval_ms - elapsed)).await;
                    }
                    *state = (Instant::now(), interval_ms);
                }

                let steps_str = steps.join("\n- ");
                let endpoint_spec_str = serde_json::to_string_pretty(&endpoint_spec)
                    .unwrap_or_else(|_| "{}".to_string());

                let user_prompt = render_template(
                    &template,
                    &[
                        ("test_name", &scenario_name),
                        ("steps", &steps_str),
                        ("description", &description),
                        ("endpoint_spec", &endpoint_spec_str),
                    ],
                );

                let chat_req = ChatRequest::new(vec![
                    ChatMessage::system(&system),
                    ChatMessage::user(user_prompt),
                ]);

                let result = client.exec_chat(&model, chat_req, None).await;

                let code = match result {
                    Ok(response) => {
                        let raw_code = response.first_text().unwrap_or_default();
                        let clean_code = raw_code
                            .trim()
                            .trim_start_matches("```javascript")
                            .trim_start_matches("```js")
                            .trim_start_matches("```")
                            .trim_end_matches("```")
                            .trim()
                            .to_string();
                        Ok(clean_code)
                    }
                    Err(e) => Err(anyhow::anyhow!("LLM call failed: {}", e)),
                };

                let _ = tx.send(E2EScriptResult { name: scenario_name, code });
            });

            handles.push(handle);
        }

        drop(tx);

        let mut scripts = Vec::new();
        let mut success_count = 0;
        let mut fail_count = 0;
        let mut written_count = 0;
        let mut skipped_count = 0;

        while let Some(result) = rx.recv().await {
            match result.code {
                Ok(code) => {
                    // Call the callback to write the file immediately
                    if let Some(ref callback) = on_script {
                        match callback(&result.name, &code) {
                            Ok(true) => {
                                println!("‚úÖ {} (written)", result.name);
                                written_count += 1;
                            }
                            Ok(false) => {
                                println!("‚è≠Ô∏è  {} (skipped)", result.name);
                                skipped_count += 1;
                            }
                            Err(e) => {
                                println!("‚ö†Ô∏è  {} - Write error: {}", result.name, e);
                            }
                        }
                    } else {
                        println!("‚úÖ {}", result.name);
                    }
                    success_count += 1;
                    scripts.push((result.name, code));
                }
                Err(e) => {
                    println!("‚ùå {} - Error: {}", result.name, e);
                    fail_count += 1;
                }
            }
        }

        for handle in handles {
            let _ = handle.await;
        }

        (scripts, success_count, fail_count, written_count, skipped_count)
    });

    println!(
        "\nüìä E2E Scripts: {} generated, {} failed",
        success_count, fail_count
    );
    
    if on_script.is_some() {
        println!("üíæ Files: {} written, {} skipped", written_count, skipped_count);
    }

    Ok(scripts)
}

/// Build scripts individually (one LLM call per test)
async fn build_scripts_individual(
    client: &Client,
    loader: &PromptLoader,
    model_name: &str,
    plan: &TestPlan,
    effective_workers: usize,
    rpm: u32,
    on_script: &Option<ScriptCallback>,
) -> (Vec<(String, String)>, usize, usize, usize, usize) {
    use std::sync::Arc;
    use tokio::sync::{mpsc, Mutex, Semaphore};
    use tokio::time::{Duration, Instant};

    let system_prompt = loader.builder_system();
    let user_template = loader.builder_user();

    let semaphore = Arc::new(Semaphore::new(effective_workers));
    
    let rate_limiter = if rpm > 0 {
        let interval_ms = 60_000 / rpm as u64;
        Some(Arc::new(Mutex::new((Instant::now(), interval_ms))))
    } else {
        None
    };

    let (tx, mut rx) = mpsc::unbounded_channel::<ScriptResult>();
    let mut handles = Vec::new();

    for test in &plan.tests {
        let client = client.clone();
        let model = model_name.to_string();
        let system = system_prompt.clone();
        let template = user_template.clone();
        let test_name = test.name.clone();
        let endpoint_spec = test.endpoint_spec.clone();
        let sem = Arc::clone(&semaphore);
        let rl = rate_limiter.clone();
        let tx = tx.clone();

        let handle = tokio::spawn(async move {
            let _permit = sem.acquire().await.unwrap();

            if let Some(ref limiter) = rl {
                let mut state = limiter.lock().await;
                let (last_time, interval_ms) = *state;
                let elapsed = last_time.elapsed().as_millis() as u64;
                if elapsed < interval_ms {
                    tokio::time::sleep(Duration::from_millis(interval_ms - elapsed)).await;
                }
                *state = (Instant::now(), interval_ms);
            }

            let endpoint_spec_str = serde_json::to_string_pretty(&endpoint_spec)
                .unwrap_or_else(|_| "{}".to_string());

            let user_prompt = render_template(
                &template,
                &[
                    ("test_name", &test_name),
                    ("endpoint_spec", &endpoint_spec_str),
                ],
            );

            let chat_req = ChatRequest::new(vec![
                ChatMessage::system(&system),
                ChatMessage::user(user_prompt),
            ]);

            let result = client.exec_chat(&model, chat_req, None).await;

            let code = match result {
                Ok(response) => {
                    let raw_code = response.first_text().unwrap_or_default();
                    let clean_code = raw_code
                        .trim()
                        .trim_start_matches("```javascript")
                        .trim_start_matches("```js")
                        .trim_start_matches("```")
                        .trim_end_matches("```")
                        .trim()
                        .to_string();
                    Ok(clean_code)
                }
                Err(e) => Err(anyhow::anyhow!("LLM call failed: {}", e)),
            };

            let _ = tx.send(ScriptResult { test_name, code });
        });

        handles.push(handle);
    }

    drop(tx);

    let mut scripts = Vec::new();
    let mut success_count = 0;
    let mut fail_count = 0;
    let mut written_count = 0;
    let mut skipped_count = 0;

    while let Some(result) = rx.recv().await {
        match result.code {
            Ok(code) => {
                // Call the callback to write the file immediately
                if let Some(ref callback) = on_script {
                    match callback(&result.test_name, &code) {
                        Ok(true) => {
                            println!("‚úÖ {} (written)", result.test_name);
                            written_count += 1;
                        }
                        Ok(false) => {
                            println!("‚è≠Ô∏è  {} (skipped)", result.test_name);
                            skipped_count += 1;
                        }
                        Err(e) => {
                            println!("‚ö†Ô∏è  {} - Write error: {}", result.test_name, e);
                        }
                    }
                } else {
                    println!("‚úÖ {}", result.test_name);
                }
                success_count += 1;
                scripts.push((result.test_name, code));
            }
            Err(e) => {
                println!("‚ùå {} - Error: {}", result.test_name, e);
                fail_count += 1;
            }
        }
    }

    for handle in handles {
        let _ = handle.await;
    }

    (scripts, success_count, fail_count, written_count, skipped_count)
}

/// Build scripts in batches (multiple tests per LLM call)
async fn build_scripts_batched(
    client: &Client,
    loader: &PromptLoader,
    model_name: &str,
    plan: &TestPlan,
    batch_size: usize,
    effective_workers: usize,
    rpm: u32,
    on_script: &Option<ScriptCallback>,
) -> (Vec<(String, String)>, usize, usize, usize, usize) {
    use std::sync::Arc;
    use tokio::sync::{mpsc, Mutex, Semaphore};
    use tokio::time::{Duration, Instant};

    let system_prompt = loader.builder_batch_system();
    let user_template = loader.builder_batch_user();

    // Group tests by endpoint (tests for same endpoint share context)
    // Then create batches respecting the batch_size limit
    let mut batches: Vec<TestBatch> = Vec::new();
    let mut current_batch: Vec<(String, serde_json::Value)> = Vec::new();

    for test in &plan.tests {
        current_batch.push((test.name.clone(), test.endpoint_spec.clone()));
        
        if current_batch.len() >= batch_size {
            batches.push(TestBatch { tests: current_batch });
            current_batch = Vec::new();
        }
    }
    
    // Don't forget the last partial batch
    if !current_batch.is_empty() {
        batches.push(TestBatch { tests: current_batch });
    }

    let num_batches = batches.len();
    let semaphore = Arc::new(Semaphore::new(effective_workers));
    
    let rate_limiter = if rpm > 0 {
        let interval_ms = 60_000 / rpm as u64;
        Some(Arc::new(Mutex::new((Instant::now(), interval_ms))))
    } else {
        None
    };

    let (tx, mut rx) = mpsc::unbounded_channel::<BatchScriptResult>();
    let mut handles = Vec::new();

    for batch in batches {
        let client = client.clone();
        let model = model_name.to_string();
        let system = system_prompt.clone();
        let template = user_template.clone();
        let sem = Arc::clone(&semaphore);
        let rl = rate_limiter.clone();
        let tx = tx.clone();
        let batch_clone = batch.clone();

        let handle = tokio::spawn(async move {
            let _permit = sem.acquire().await.unwrap();

            if let Some(ref limiter) = rl {
                let mut state = limiter.lock().await;
                let (last_time, interval_ms) = *state;
                let elapsed = last_time.elapsed().as_millis() as u64;
                if elapsed < interval_ms {
                    tokio::time::sleep(Duration::from_millis(interval_ms - elapsed)).await;
                }
                *state = (Instant::now(), interval_ms);
            }

            // Build the tests JSON for the batch
            let tests_json: Vec<serde_json::Value> = batch.tests.iter()
                .map(|(name, spec)| {
                    serde_json::json!({
                        "test_name": name,
                        "endpoint_spec": spec
                    })
                })
                .collect();
            
            let tests_str = serde_json::to_string_pretty(&tests_json)
                .unwrap_or_else(|_| "[]".to_string());
            
            let user_prompt = render_template(&template, &[("tests", &tests_str)]);

            let chat_req = ChatRequest::new(vec![
                ChatMessage::system(&system),
                ChatMessage::user(user_prompt),
            ]);

            let result = client.exec_chat(&model, chat_req, None).await;

            let results = match result {
                Ok(response) => {
                    let content = response.first_text().unwrap_or_default();
                    parse_batched_scripts(&content)
                }
                Err(e) => Err(anyhow::anyhow!("LLM call failed: {}", e)),
            };

            let _ = tx.send(BatchScriptResult {
                batch: batch_clone,
                results,
            });
        });

        handles.push(handle);
    }

    drop(tx);

    let mut scripts = Vec::new();
    let mut success_count = 0;
    let mut fail_count = 0;
    let mut written_count = 0;
    let mut skipped_count = 0;
    let mut batches_completed = 0;

    while let Some(batch_result) = rx.recv().await {
        batches_completed += 1;
        match batch_result.results {
            Ok(results_map) => {
                for (test_name, _) in &batch_result.batch.tests {
                    if let Some(code) = results_map.get(test_name) {
                        // Clean up the code (remove any remaining markdown fences)
                        let clean_code = code
                            .trim()
                            .trim_start_matches("```javascript")
                            .trim_start_matches("```js")
                            .trim_start_matches("```")
                            .trim_end_matches("```")
                            .trim()
                            .to_string();
                        
                        // Call the callback to write the file immediately
                        if let Some(ref callback) = on_script {
                            match callback(test_name, &clean_code) {
                                Ok(true) => {
                                    println!("‚úÖ {} (written)", test_name);
                                    written_count += 1;
                                }
                                Ok(false) => {
                                    println!("‚è≠Ô∏è  {} (skipped)", test_name);
                                    skipped_count += 1;
                                }
                                Err(e) => {
                                    println!("‚ö†Ô∏è  {} - Write error: {}", test_name, e);
                                }
                            }
                        } else {
                            println!("‚úÖ {}", test_name);
                        }
                        success_count += 1;
                        scripts.push((test_name.clone(), clean_code));
                    } else {
                        println!("‚ö†Ô∏è  {} - Missing from batch response", test_name);
                        fail_count += 1;
                    }
                }
                println!("   üì¶ Batch {}/{} complete", batches_completed, num_batches);
            }
            Err(e) => {
                println!("‚ùå Batch {}/{} failed ({} tests): {}", batches_completed, num_batches, batch_result.batch.tests.len(), e);
                for (test_name, _) in &batch_result.batch.tests {
                    println!("   ‚Ä¢ {}", test_name);
                }
                fail_count += batch_result.batch.tests.len();
            }
        }
    }

    for handle in handles {
        let _ = handle.await;
    }

    (scripts, success_count, fail_count, written_count, skipped_count)
}

/// Parse LLM response content to extract clean JSON
#[allow(dead_code)]
pub fn parse_llm_json_response(content: &str) -> String {
    content
        .trim()
        .trim_start_matches("```json")
        .trim_start_matches("```")
        .trim_end_matches("```")
        .trim()
        .to_string()
}

/// Parse LLM response content to extract clean code
#[allow(dead_code)]
pub fn parse_llm_code_response(content: &str) -> String {
    content
        .trim()
        .trim_start_matches("```javascript")
        .trim_start_matches("```js")
        .trim_start_matches("```")
        .trim_end_matches("```")
        .trim()
        .to_string()
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::tempdir;

    // ============================================
    // TestScenario JSON Parsing Tests
    // ============================================

    #[test]
    fn test_parse_test_scenario_minimal() {
        let json = r#"{
            "name": "Test Login",
            "steps": ["Step 1", "Step 2"]
        }"#;
        let scenario: TestScenario = serde_json::from_str(json).unwrap();
        assert_eq!(scenario.name, "Test Login");
        assert_eq!(scenario.steps.len(), 2);
        assert_eq!(scenario.priority, "medium"); // default
        assert!(scenario.tags.is_empty()); // default
    }

    #[test]
    fn test_parse_test_scenario_full() {
        let json = r#"{
            "name": "Test User Registration",
            "steps": ["Create user", "Verify email", "Login"],
            "priority": "high",
            "tags": ["auth", "user", "critical"]
        }"#;
        let scenario: TestScenario = serde_json::from_str(json).unwrap();
        assert_eq!(scenario.name, "Test User Registration");
        assert_eq!(scenario.steps.len(), 3);
        assert_eq!(scenario.priority, "high");
        assert_eq!(scenario.tags, vec!["auth", "user", "critical"]);
    }

    #[test]
    fn test_parse_test_scenario_array() {
        let json = r#"[
            {"name": "Test 1", "steps": ["step 1"]},
            {"name": "Test 2", "steps": ["step 2"], "priority": "low"},
            {"name": "Test 3", "steps": ["step 3"], "tags": ["smoke"]}
        ]"#;
        let scenarios: Vec<TestScenario> = serde_json::from_str(json).unwrap();
        assert_eq!(scenarios.len(), 3);
        assert_eq!(scenarios[0].name, "Test 1");
        assert_eq!(scenarios[1].priority, "low");
        assert_eq!(scenarios[2].tags, vec!["smoke"]);
    }

    #[test]
    fn test_parse_test_scenario_empty_steps() {
        let json = r#"{"name": "Empty Test", "steps": []}"#;
        let scenario: TestScenario = serde_json::from_str(json).unwrap();
        assert!(scenario.steps.is_empty());
    }

    #[test]
    fn test_parse_test_scenario_serialization() {
        let scenario = TestScenario {
            name: "Test Serialization".to_string(),
            steps: vec!["Step 1".to_string(), "Step 2".to_string()],
            priority: "high".to_string(),
            tags: vec!["tag1".to_string()],
        };
        let json = serde_json::to_string(&scenario).unwrap();
        assert!(json.contains("\"name\":\"Test Serialization\""));
        assert!(json.contains("\"priority\":\"high\""));
    }

    #[test]
    fn test_test_scenario_clone() {
        let scenario = TestScenario {
            name: "Original".to_string(),
            steps: vec!["Step".to_string()],
            priority: "high".to_string(),
            tags: vec!["tag".to_string()],
        };
        let cloned = scenario.clone();
        assert_eq!(cloned.name, scenario.name);
        assert_eq!(cloned.steps, scenario.steps);
    }

    // ============================================
    // Template Rendering Tests
    // ============================================

    #[test]
    fn test_render_template_single_var() {
        let template = "Hello, {{name}}!";
        let result = render_template(template, &[("name", "World")]);
        assert_eq!(result, "Hello, World!");
    }

    #[test]
    fn test_render_template_multiple_vars() {
        let template = "{{greeting}}, {{name}}! Your score is {{score}}.";
        let result = render_template(
            template,
            &[("greeting", "Hello"), ("name", "Alice"), ("score", "100")],
        );
        assert_eq!(result, "Hello, Alice! Your score is 100.");
    }

    #[test]
    fn test_render_template_no_vars() {
        let template = "No variables here.";
        let result = render_template(template, &[]);
        assert_eq!(result, "No variables here.");
    }

    #[test]
    fn test_render_template_missing_var() {
        let template = "Hello, {{name}}!";
        let result = render_template(template, &[]); // No replacement provided
        assert_eq!(result, "Hello, {{name}}!"); // Should remain unchanged
    }

    #[test]
    fn test_render_template_duplicate_var() {
        let template = "{{name}} met {{name}} at the {{name}} store.";
        let result = render_template(template, &[("name", "Bob")]);
        assert_eq!(result, "Bob met Bob at the Bob store.");
    }

    #[test]
    fn test_render_template_empty_value() {
        let template = "Value: {{value}}";
        let result = render_template(template, &[("value", "")]);
        assert_eq!(result, "Value: ");
    }

    #[test]
    fn test_render_template_multiline() {
        let template = "Line 1: {{var1}}\nLine 2: {{var2}}\nLine 3: {{var3}}";
        let result = render_template(
            template,
            &[("var1", "A"), ("var2", "B"), ("var3", "C")],
        );
        assert_eq!(result, "Line 1: A\nLine 2: B\nLine 3: C");
    }

    #[test]
    fn test_render_template_json_content() {
        let template = "Spec: {{spec}}";
        let json_content = r#"{"openapi": "3.0.0", "info": {"title": "API"}}"#;
        let result = render_template(template, &[("spec", json_content)]);
        assert!(result.contains("openapi"));
    }

    #[test]
    fn test_render_template_special_chars_in_value() {
        let template = "Data: {{data}}";
        let result = render_template(template, &[("data", "<html>&amp;\"quotes\"</html>")]);
        assert_eq!(result, "Data: <html>&amp;\"quotes\"</html>");
    }

    // ============================================
    // LLM Response Parsing Tests
    // ============================================

    #[test]
    fn test_parse_llm_json_response_clean() {
        let content = r#"[{"name": "test", "steps": ["step1"]}]"#;
        let result = parse_llm_json_response(content);
        assert_eq!(result, r#"[{"name": "test", "steps": ["step1"]}]"#);
    }

    #[test]
    fn test_parse_llm_json_response_with_markdown() {
        let content = "```json\n[{\"name\": \"test\"}]\n```";
        let result = parse_llm_json_response(content);
        assert_eq!(result, r#"[{"name": "test"}]"#);
    }

    #[test]
    fn test_parse_llm_json_response_with_plain_markdown() {
        let content = "```\n{\"key\": \"value\"}\n```";
        let result = parse_llm_json_response(content);
        assert_eq!(result, r#"{"key": "value"}"#);
    }

    #[test]
    fn test_parse_llm_json_response_with_whitespace() {
        let content = "   \n  ```json\n{\"test\": true}\n```  \n  ";
        let result = parse_llm_json_response(content);
        assert_eq!(result, r#"{"test": true}"#);
    }

    #[test]
    fn test_parse_llm_code_response_javascript() {
        let content = "```javascript\nconst x = 1;\nconsole.log(x);\n```";
        let result = parse_llm_code_response(content);
        assert_eq!(result, "const x = 1;\nconsole.log(x);");
    }

    #[test]
    fn test_parse_llm_code_response_js() {
        let content = "```js\nfunction test() { return true; }\n```";
        let result = parse_llm_code_response(content);
        assert_eq!(result, "function test() { return true; }");
    }

    #[test]
    fn test_parse_llm_code_response_plain() {
        let content = "```\nlet result = http.get('/api');\n```";
        let result = parse_llm_code_response(content);
        assert_eq!(result, "let result = http.get('/api');");
    }

    #[test]
    fn test_parse_llm_code_response_no_markers() {
        let content = "const response = http.get('/health');";
        let result = parse_llm_code_response(content);
        assert_eq!(result, "const response = http.get('/health');");
    }

    // ============================================
    // PromptLoader Tests
    // ============================================

    #[test]
    fn test_prompt_loader_default_prompts() {
        let loader = PromptLoader::new(None);

        // Should return embedded defaults
        let planner_system = loader.planner_system();
        let planner_user = loader.planner_user();
        let builder_system = loader.builder_system();
        let builder_user = loader.builder_user();

        // Verify defaults are non-empty
        assert!(!planner_system.is_empty());
        assert!(!planner_user.is_empty());
        assert!(!builder_system.is_empty());
        assert!(!builder_user.is_empty());
    }

    #[test]
    fn test_prompt_loader_custom_directory() {
        let dir = tempdir().unwrap();
        let custom_prompt = "Custom system prompt content";

        // Create custom prompt file
        let prompt_path = dir.path().join("planner_system.md");
        fs::write(&prompt_path, custom_prompt).unwrap();

        let loader = PromptLoader::new(Some(dir.path().to_string_lossy().to_string()));
        let result = loader.planner_system();

        assert_eq!(result, custom_prompt);
    }

    #[test]
    fn test_prompt_loader_fallback_to_default() {
        let dir = tempdir().unwrap();
        // Don't create any files - should fall back to defaults

        let loader = PromptLoader::new(Some(dir.path().to_string_lossy().to_string()));
        let result = loader.planner_system();

        // Should return embedded default
        assert!(!result.is_empty());
    }

    #[test]
    fn test_prompt_loader_partial_custom() {
        let dir = tempdir().unwrap();

        // Only create one custom prompt
        let custom_content = "My custom builder system prompt";
        fs::write(dir.path().join("builder_system.md"), custom_content).unwrap();

        let loader = PromptLoader::new(Some(dir.path().to_string_lossy().to_string()));

        // This should use custom
        assert_eq!(loader.builder_system(), custom_content);

        // These should use defaults
        assert!(!loader.planner_system().is_empty());
        assert!(!loader.planner_user().is_empty());
        assert!(!loader.builder_user().is_empty());
    }

    #[test]
    fn test_prompt_loader_nonexistent_directory() {
        let loader = PromptLoader::new(Some("/nonexistent/path/to/prompts".to_string()));

        // Should fall back to defaults
        assert!(!loader.planner_system().is_empty());
    }

    // ============================================
    // Default Priority Tests
    // ============================================

    #[test]
    fn test_default_priority() {
        assert_eq!(default_priority(), "medium");
    }

    // ============================================
    // Complex JSON Parsing Tests
    // ============================================

    #[test]
    fn test_parse_complex_scenarios() {
        let json = r#"[
            {
                "name": "API Health Check",
                "steps": [
                    "Send GET request to /health",
                    "Verify response status is 200",
                    "Check response body contains 'ok'"
                ],
                "priority": "critical",
                "tags": ["health", "smoke", "monitoring"]
            },
            {
                "name": "User CRUD Operations",
                "steps": [
                    "Create a new user with POST /users",
                    "Read user with GET /users/{id}",
                    "Update user with PUT /users/{id}",
                    "Delete user with DELETE /users/{id}",
                    "Verify user no longer exists"
                ],
                "priority": "high",
                "tags": ["crud", "user-management"]
            }
        ]"#;

        let scenarios: Vec<TestScenario> = serde_json::from_str(json).unwrap();
        assert_eq!(scenarios.len(), 2);
        assert_eq!(scenarios[0].steps.len(), 3);
        assert_eq!(scenarios[1].steps.len(), 5);
        assert_eq!(scenarios[0].tags.len(), 3);
    }

    #[test]
    fn test_parse_scenario_with_unicode() {
        let json = r#"{
            "name": "ÂõΩÈôÖÂåñÊµãËØï - Internationalization Test",
            "steps": ["Ê≠•È™§‰∏Ä - Step 1", "√âtape 2", "Schritt 3 üöÄ"],
            "priority": "medium",
            "tags": ["i18n", "unicode", "Â§öËØ≠Ë®Ä"]
        }"#;

        let scenario: TestScenario = serde_json::from_str(json).unwrap();
        assert!(scenario.name.contains("ÂõΩÈôÖÂåñ"));
        assert!(scenario.steps[2].contains("üöÄ"));
        assert!(scenario.tags.contains(&"Â§öËØ≠Ë®Ä".to_string()));
    }

    #[test]
    fn test_parse_scenario_with_special_chars_in_steps() {
        let json = r#"{
            "name": "Test Special Characters",
            "steps": [
                "Send request with query param ?foo=bar&baz=qux",
                "Verify response contains \"escaped quotes\"",
                "Check header X-Custom: value/with/slashes"
            ]
        }"#;

        let scenario: TestScenario = serde_json::from_str(json).unwrap();
        assert!(scenario.steps[0].contains("&"));
        assert!(scenario.steps[1].contains("\""));
        assert!(scenario.steps[2].contains("/"));
    }
}

